{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text)  \n",
    "Jens Albrecht, Sidharth Ramachandran, Christian Winkler\n",
    "\n",
    "# Chapter 6:<div class='tocSkip'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use classification algorithms to label text into multiple categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark<div class='tocSkip'/>\n",
    "\n",
    "The code in this notebook differs slightly from the printed book. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
    "\n",
    "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book.\n",
    "\n",
    "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
    "\n",
    "All of this is done to simplify the code in the book and put the focus on the important parts instead of formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup<div class='tocSkip'/>\n",
    "\n",
    "Set directory locations. If working on Google Colab: copy files and install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working on a local system.\n",
      "Files will be searched relative to \"..\".\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master'\n",
    "    os.system(f'wget {GIT_ROOT}/ch06/setup.py')\n",
    "\n",
    "%run -i setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Settings<div class=\"tocSkip\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"$BASE_DIR/settings.py\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import html \n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll learn and what we will build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Java Development Tools Bug Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/jdt-bugs-dataset/eclipse_jdt.csv.gz')\n",
    "print (df.columns)\n",
    "df[['Issue_id','Priority','Component','Title','Description']].sample(2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Duplicated_issue']) ###\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.sample(1, random_state=123).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Priority'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Component'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprint: Building a Text Classification system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Title','Description','Priority']]\n",
    "df = df.dropna()\n",
    "df['text'] = df['Title'] + ' ' + df['Description']\n",
    "df = df.drop(columns=['Title','Description'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean)\n",
    "df = df[df['text'].str.len() > 50]\n",
    "df.sample(2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n",
    "                                                    df['Priority'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df['Priority'])\n",
    "\n",
    "print('Size of Training Data ', X_train.shape[0])\n",
    "print('Size of Test Data ', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Training the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df = 10, ngram_range=(1,2), stop_words=\"english\")\n",
    "X_train_tf = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LinearSVC(random_state=0, tol=1e-5)\n",
    "model1.fit(X_train_tf, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model1.predict(X_test_tf)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred_baseline = clf.predict(X_test)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model1.predict(X_test_tf)\n",
    "confusion_matrix(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model1,X_test_tf,\n",
    "                      Y_test, values_format='d',\n",
    "                      cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter bug reports with priority P3 and sample 4000 rows from it\n",
    "df_sampleP3 = df[df['Priority'] == 'P3'].sample(n=4000, random_state=123)\n",
    "\n",
    "# Create a separate dataframe containing all other bug reports\n",
    "df_sampleRest = df[df['Priority'] != 'P3']\n",
    "\n",
    "# Concatenate the two dataframes to create the new balanced bug reports dataset\n",
    "df_balanced = pd.concat([df_sampleRest, df_sampleP3])\n",
    "\n",
    "# Check the status of the class imbalance\n",
    "df_balanced['Priority'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Blueprint for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the balanced dataframe\n",
    "\n",
    "df = df_balanced[['text', 'Priority']]\n",
    "df = df.dropna()\n",
    "\n",
    "# Step 1 - Data Preparation\n",
    "\n",
    "df['text'] = df['text'].apply(clean)\n",
    "\n",
    "# Step 2 - Train-Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n",
    "                                                    df['Priority'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df['Priority'])\n",
    "print('Size of Training Data ', X_train.shape[0])\n",
    "print('Size of Test Data ', X_test.shape[0])\n",
    "\n",
    "# Step 3 - Training the Machine Learning model\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2), stop_words=\"english\")\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "\n",
    "model1 = LinearSVC(random_state=0, tol=1e-5)\n",
    "model1.fit(X_train_tf, Y_train)\n",
    "\n",
    "# Step 4 - Model Evaluation\n",
    "\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "Y_pred = model1.predict(X_test_tf)\n",
    "print('Accuracy Score - ', accuracy_score(Y_test, Y_pred))\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DummyClassifier(strategy='stratified', random_state=21)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred_baseline = clf.predict(X_test)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe combining the Title and Description, \n",
    "## Actual and Predicted values that we can explore\n",
    "frame = { 'text': X_test, 'actual': Y_test, 'predicted': Y_pred }\n",
    "result = pd.DataFrame(frame)\n",
    "\n",
    "result[((result['actual'] == 'P1') | (result['actual'] == 'P2')) &\n",
    "       (result['actual'] == result['predicted'])].sample(2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[((result['actual'] == 'P1') | (result['actual'] == 'P2')) &\n",
    "       (result['actual'] != result['predicted'])].sample(2, random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df = 10, ngram_range=(1,2), stop_words=\"english\")\n",
    "df_tf = tfidf.fit_transform(df['text']).toarray()\n",
    "\n",
    "# Cross Validation with 5 folds\n",
    "\n",
    "scores = cross_val_score(estimator=model1,\n",
    "                         X=df_tf,\n",
    "                         y=df['Priority'],\n",
    "                         cv=5)\n",
    "\n",
    "print (\"Validation scores from each iteration of the cross validation \", scores)\n",
    "print (\"Mean value across of validation scores \", scores.mean())\n",
    "print (\"Standard deviation of validation scores \", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline = Pipeline(\n",
    "    steps=[('tfidf', TfidfVectorizer(\n",
    "        stop_words=\"english\")), ('model',\n",
    "                                 LinearSVC(random_state=21, tol=1e-5))])\n",
    "\n",
    "grid_param = [{\n",
    "    'tfidf__min_df': [5, 10],\n",
    "    'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
    "    'model__penalty': ['l2'],\n",
    "    'model__loss': ['hinge'],\n",
    "    'model__max_iter': [10000]\n",
    "}, {\n",
    "    'tfidf__min_df': [5, 10],\n",
    "    'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
    "    'model__C': [1, 10],\n",
    "    'model__tol': [1e-2, 1e-3]\n",
    "}]\n",
    "\n",
    "gridSearchProcessor = GridSearchCV(estimator=training_pipeline,\n",
    "                                   param_grid=grid_param,\n",
    "                                   cv=5)\n",
    "gridSearchProcessor.fit(df['text'], df['Priority'])\n",
    "\n",
    "best_params = gridSearchProcessor.best_params_\n",
    "print(\"Best alpha parameter identified by grid search \", best_params)\n",
    "\n",
    "best_result = gridSearchProcessor.best_score_\n",
    "print(\"Best result identified by grid search \", best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_results = pd.DataFrame(gridSearchProcessor.cv_results_)\n",
    "gridsearch_results[['rank_test_score', 'mean_test_score',\n",
    "                    'params']].sort_values(by=['rank_test_score'])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprint recap and conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag that determines the choice of SVC (True) and LinearSVC (False)\n",
    "runSVC = True\n",
    "\n",
    "# Loading the dataframe\n",
    "\n",
    "df = pd.read_csv('../data/jdt-bugs-dataset/eclipse_jdt.csv.gz')\n",
    "df = df[['Title', 'Description', 'Component']]\n",
    "df = df.dropna()\n",
    "df['text'] = df['Title'] + df['Description']\n",
    "df = df.drop(columns=['Title', 'Description'])\n",
    "\n",
    "# Step 1 - Data Preparation\n",
    "df['text'] = df['text'].apply(clean)\n",
    "df = df[df['text'].str.len() > 50]\n",
    "\n",
    "if (runSVC):\n",
    "    # Sample the data when running SVC to ensure reasonable run-times\n",
    "    df = df.groupby('Component', as_index=False).apply(pd.DataFrame.sample,\n",
    "                                                       random_state=42,\n",
    "                                                       frac=.2)\n",
    "\n",
    "# Step 2 - Train-Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n",
    "                                                    df['Component'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df['Component'])\n",
    "print('Size of Training Data ', X_train.shape[0])\n",
    "print('Size of Test Data ', X_test.shape[0])\n",
    "\n",
    "# Step 3 - Training the Machine Learning model\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "if (runSVC):\n",
    "    model = SVC(random_state=42, probability=True)\n",
    "    grid_param = [{\n",
    "        'tfidf__min_df': [5, 10],\n",
    "        'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
    "        'model__C': [1, 100],\n",
    "        'model__kernel': ['linear']\n",
    "    }]\n",
    "else:\n",
    "    model = LinearSVC(random_state=42, tol=1e-5)\n",
    "    grid_param = {\n",
    "        'tfidf__min_df': [5, 10],\n",
    "        'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
    "        'model__C': [1, 100],\n",
    "        'model__loss': ['hinge']\n",
    "    }\n",
    "\n",
    "training_pipeline = Pipeline(\n",
    "    steps=[('tfidf', TfidfVectorizer(stop_words=\"english\")), ('model', model)])\n",
    "\n",
    "gridSearchProcessor = GridSearchCV(estimator=training_pipeline,\n",
    "                                   param_grid=grid_param,\n",
    "                                   cv=5)\n",
    "\n",
    "gridSearchProcessor.fit(X_train, Y_train)\n",
    "\n",
    "best_params = gridSearchProcessor.best_params_\n",
    "print(\"Best alpha parameter identified by grid search \", best_params)\n",
    "\n",
    "best_result = gridSearchProcessor.best_score_\n",
    "print(\"Best result identified by grid search \", best_result)\n",
    "\n",
    "best_model = gridSearchProcessor.best_estimator_\n",
    "\n",
    "# Step 4 - Model Evaluation\n",
    "\n",
    "Y_pred = best_model.predict(X_test)\n",
    "print('Accuracy Score - ', accuracy_score(Y_test, Y_pred))\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent', random_state=21)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred_baseline = clf.predict(X_test)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe combining the Title and Description, \n",
    "## Actual and Predicted values that we can explore\n",
    "frame = { 'text': X_test, 'actual': Y_test, 'predicted': Y_pred } \n",
    "result = pd.DataFrame(frame)\n",
    "\n",
    "result[result['actual'] == result['predicted']].sample(2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result['actual'] != result['predicted']].sample(2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Remarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
