{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text)  \n",
                "Jens Albrecht, Sidharth Ramachandran, Christian Winkler\n",
                "\n",
                "# Chapter 10:<div class='tocSkip'/>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exploring Semantic Relationships with Word Embeddings"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Remark<div class='tocSkip'/>\n",
                "\n",
                "The code in this notebook differs slightly from the printed book. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
                "\n",
                "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book.\n",
                "\n",
                "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
                "\n",
                "All of this is done to simplify the code in the book and put the focus on the important parts instead of formatting."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup<div class='tocSkip'/>\n",
                "\n",
                "Set directory locations. If working on Google Colab: copy files and install required libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys, os\n",
                "ON_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if ON_COLAB:\n",
                "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master'\n",
                "    os.system(f'wget {GIT_ROOT}/ch10/setup.py')\n",
                "\n",
                "%run -i setup.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Python Settings<div class=\"tocSkip\"/>\n",
                "\n",
                "Common imports, defaults for formatting in Matplotlib, Pandas etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%run \"$BASE_DIR/settings.py\"\n",
                "\n",
                "%reload_ext autoreload\n",
                "%autoreload 2\n",
                "%config InlineBackend.figure_format = 'png'\n",
                "\n",
                "# set precision for similarity values\n",
                "%precision 3 \n",
                "np.set_printoptions(suppress=True) # no scientific for small numbers\n",
                "\n",
                "# path to import blueprints packages\n",
                "sys.path.append(BASE_DIR + '/packages')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# to ensure identical word2vec models in different trainings\n",
                "# os.environ['PYTHONHASHSEED']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## What you will learn and what we will build\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Case for Semantic Embeddings\n",
                "## Word Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analogy Reasoning with Word Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Types of Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Word2Vec\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### GloVe\n",
                "### FastText\n",
                "### Deep Contextualized Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Blueprint: Similarity Queries on Pre-Trained Models\n",
                "## Loading a Pretrained Model\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ['GENSIM_DATA_DIR'] = './models'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# pandas number format\n",
                "pd.options.display.float_format = '{:.0f}'.format"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gensim.downloader as api\n",
                "\n",
                "info_df = pd.DataFrame.from_dict(api.info()['models'], orient='index')\n",
                "info_df[['file_size', 'base_dataset', 'parameters']].head(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# full list of columns\n",
                "info_df.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.options.display.float_format = '{:.2f}'.format"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = api.load(\"glove-wiki-gigaword-50\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Similarity Queries\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%precision 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "v_king = model['king']\n",
                "v_queen = model['queen']\n",
                "\n",
                "print(\"Vector size:\", model.vector_size)\n",
                "print(\"v_king  =\", v_king[:10])\n",
                "print(\"v_queen =\", v_queen[:10])\n",
                "print(\"similarity:\", model.similarity('king', 'queen'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%precision 3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.most_similar('king', topn=3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "v_lion = model['lion']\n",
                "v_nano = model['nanotechnology']\n",
                "\n",
                "model.cosine_similarities(v_king, [v_queen, v_lion, v_nano])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.most_similar(positive=['paris', 'germany'], negative=['france'], topn=3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.most_similar(positive=['france', 'capital'], topn=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.most_similar(positive=['greece', 'capital'], topn=3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Blueprints for Training and Evaluation of Your Own Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Preparation\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "db_name = f\"{BASE_DIR}/data/reddit-selfposts/reddit-selfposts-ch10.db\"\n",
                "con = sqlite3.connect(db_name)\n",
                "df = pd.read_sql(\"select subreddit, lemmas, text from posts_nlp\", con)\n",
                "con.close()\n",
                "\n",
                "df['lemmas'] = df['lemmas'].str.lower().str.split() # lower case tokens\n",
                "sents = df['lemmas'] # our training \"sentences\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Phrases\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gensim.models.phrases import Phrases, npmi_scorer\n",
                "\n",
                "phrases = Phrases(sents, min_count=10, threshold=0.3, \n",
                "                  delimiter=b'-', scoring=npmi_scorer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sent = \"I had to replace the timing belt in my mercedes c300\".split()\n",
                "phrased = phrases[sent]\n",
                "print('|'.join(phrased))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "phrase_df = pd.DataFrame(phrases.export_phrases(sents), \n",
                "                         columns =['phrase', 'score'])\n",
                "phrase_df = phrase_df[['phrase', 'score']].drop_duplicates() \\\n",
                "            .sort_values(by='score', ascending=False).reset_index(drop=True)\n",
                "phrase_df['phrase'] = phrase_df['phrase'].map(lambda p: p.decode('utf-8'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "phrase_df[phrase_df['phrase'].str.contains('mercedes')] .head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# show some additional phrases with score > 0.7\n",
                "phrase_df.query('score > 0.7').sample(100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logging.getLogger().setLevel(logging.WARNING) ###\n",
                "sents = df['lemmas'] ### like above\n",
                "phrases = Phrases(sents, min_count=10, threshold=0.7, \n",
                "                  delimiter=b'-', scoring=npmi_scorer)\n",
                "\n",
                "df['phrased_lemmas'] = df['lemmas'].progress_map(lambda s: phrases[s])\n",
                "sents = df['phrased_lemmas']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Blueprint: Training Models with Gensim\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# for Gensim training\n",
                "\n",
                "import logging\n",
                "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)\n",
                "logging.getLogger().setLevel(logging.INFO)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gensim.models import Word2Vec\n",
                "\n",
                "model = Word2Vec(sents,       # tokenized input sentences\n",
                "                 size=100,    # size of word vectors (default 100)\n",
                "                 window=2,    # context window size (default 5)\n",
                "                 sg=1,        # use skip-gram (default 0 = CBOW)\n",
                "                 negative=5,  # number of negative samples (default 5)\n",
                "                 min_count=5, # ignore infrequent words (default 5)\n",
                "                 workers=4,   # number of threads (default 3)\n",
                "                 iter=5)      # number of epochs (default 5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logging.getLogger().setLevel(logging.ERROR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save('./models/autos_w2v_100_2_full.bin')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = Word2Vec.load('./models/autos_w2v_100_2_full.bin')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**This takes several minutes to run.** Please be patient, you need this to continue."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gensim.models import Word2Vec, FastText\n",
                "\n",
                "model_path = './models'\n",
                "model_prefix = 'autos'\n",
                "\n",
                "param_grid = {'w2v': {'variant': ['cbow', 'sg'], 'window': [2, 5, 30]},\n",
                "              'ft': {'variant': ['sg'], 'window': [5]}}\n",
                "size = 100\n",
                "\n",
                "for algo, params in param_grid.items(): \n",
                "    print(algo) ###\n",
                "    for variant in params['variant']:\n",
                "        sg = 1 if variant == 'sg' else 0\n",
                "        for window in params['window']:\n",
                "            print(f\"  Variant: {variant}, Window: {window}, Size: {size}\") ###\n",
                "            np.random.seed(1) ### to ensure repeatability\n",
                "            if algo == 'w2v':\n",
                "                model = Word2Vec(sents, size=size, window=window, sg=sg)\n",
                "            else:\n",
                "                model = FastText(sents, size=size, window=window, sg=sg)\n",
                "\n",
                "            file_name = f\"{model_path}/{model_prefix}_{algo}_{variant}_{window}\"\n",
                "            model.wv.save_word2vec_format(file_name + '.bin', binary=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Blueprint: Evaluating Different Models\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gensim.models import KeyedVectors\n",
                "model_path = './models' ###\n",
                "\n",
                "names = ['autos_w2v_cbow_2', 'autos_w2v_sg_2', \n",
                "         'autos_w2v_sg_5', 'autos_w2v_sg_30', 'autos_ft_sg_5']\n",
                "models = {}\n",
                "\n",
                "for name in names:\n",
                "    file_name = f\"{model_path}/{name}.bin\"\n",
                "    print(f\"Loading {file_name}\") ###\n",
                "    models[name] = KeyedVectors.load_word2vec_format(file_name, binary=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compare_models(models, **kwargs):\n",
                "\n",
                "    df = pd.DataFrame()\n",
                "    for name, model in models:\n",
                "        df[name] = [f\"{word} {score:.3f}\" \n",
                "                    for word, score in model.most_similar(**kwargs)]\n",
                "    df.index = df.index + 1 # let row index start at 1\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "compare_models([(n, models[n]) for n in names], positive='bmw', topn=10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Looking for Similar Concepts\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Analogy Reasoning on our own Models\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Note** that your results may be slightly different to the ones printed in the book because of random initialization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "compare_models([(n, models[n]) for n in names], \n",
                "               positive=['f150', 'toyota'], negative=['ford'], topn=5).T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# try a different analogy\n",
                "compare_models([(n, models[n]) for n in names], \n",
                "               positive=['x3', 'audi'], negative=['bmw'], topn=5).T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# and another one\n",
                "compare_models([(n, models[n]) for n in names], \n",
                "               positive=['spark-plug'], negative=[], topn=5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Blueprints for Visualizing Embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Blueprint: Applying Dimensionality Reduction\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from umap import UMAP\n",
                "\n",
                "model = models['autos_w2v_sg_30']\n",
                "words = model.vocab\n",
                "wv = [model[word] for word in words]\n",
                "\n",
                "reducer = UMAP(n_components=2, metric='cosine', n_neighbors = 15, min_dist=0.1, random_state = 12)\n",
                "reduced_wv = reducer.fit_transform(wv)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import plotly.express as px\n",
                "px.defaults.template = \"plotly_white\" ### plotly style\n",
                "\n",
                "plot_df = pd.DataFrame.from_records(reduced_wv, columns=['x', 'y'])\n",
                "plot_df['word'] = words\n",
                "params = {'hover_data': {c: False for c in plot_df.columns}, \n",
                "          'hover_name': 'word'}\n",
                "params.update({'width': 800, 'height': 600}) ###\n",
                "\n",
                "fig = px.scatter(plot_df, x=\"x\", y=\"y\", opacity=0.3, size_max=3, **params)\n",
                "fig.update_traces(marker={'line': {'width': 0}}) ###\n",
                "fig.update_xaxes(showticklabels=False, showgrid=True, zeroline=False, visible=True) ###\n",
                "fig.update_yaxes(showticklabels=False, showgrid=True, zeroline=False, visible=True) ###\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from blueprints.embeddings import plot_embeddings\n",
                "\n",
                "model = models['autos_w2v_sg_30'] ###\n",
                "search = ['ford', 'lexus', 'vw', 'hyundai', \n",
                "          'goodyear', 'spark-plug', 'florida', 'navigation']\n",
                "\n",
                "_ = plot_embeddings(model, search, topn=50, show_all=True, labels=False, \n",
                "                algo='umap', n_neighbors=15, min_dist=0.1, random_state=12)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = models['autos_w2v_sg_30'] ###\n",
                "search = ['ford', 'bmw', 'toyota', 'tesla', 'audi', 'mercedes', 'hyundai']\n",
                "\n",
                "_ = plot_embeddings(model, search, topn=10, show_all=False, labels=True, \n",
                "    algo='umap', n_neighbors=15, min_dist=10, spread=25, random_state=7)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "_ = plot_embeddings(model, search, topn=30, n_dims=3, \n",
                "    algo='umap', n_neighbors=15, min_dist=.1, spread=40, random_state=23)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PCA plot (not in the book) - better to explain analogies:\n",
                "# difference vectors of pickup trucks \"f150\"-\"ford\", \"tacoma\"-\"toyota\" and \n",
                "# \"frontier\"-\"nissan\" are almost parallel. \n",
                "# \"x5\"-\"bmw\" is pointing to a somewhat different direction, but \"x5\" is not a pickup\n",
                "\n",
                "model = models['autos_w2v_sg_5'] \n",
                "search = ['ford', 'f150', 'toyota', 'tacoma', 'nissan', 'frontier', 'bmw', 'x5']\n",
                "_ = plot_embeddings(model, search, topn=0, algo='pca', labels=True, colors=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Blueprint: Using Tensorflow Embedding Projector\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import csv\n",
                "\n",
                "model_path = './models' ###\n",
                "name = 'autos_w2v_sg_30'\n",
                "model = models[name]\n",
                "\n",
                "with open(f'{model_path}/{name}_words.tsv', 'w', encoding='utf-8') as tsvfile:\n",
                "    tsvfile.write('\\n'.join(model.vocab))\n",
                "\n",
                "with open(f'{model_path}/{name}_vecs.tsv', 'w', encoding='utf-8') as tsvfile:\n",
                "    writer = csv.writer(tsvfile, delimiter='\\t', \n",
                "                        dialect=csv.unix_dialect, quoting=csv.QUOTE_MINIMAL)\n",
                "    for w in model.vocab:\n",
                "        _ = writer.writerow(model[w].tolist())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Blueprint: Constructing a Similarity Tree\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import networkx as nx\n",
                "from collections import deque\n",
                "\n",
                "def sim_tree(model, word, top_n, max_dist):\n",
                "\n",
                "    graph = nx.Graph()\n",
                "    graph.add_node(word, dist=0)\n",
                "\n",
                "    to_visit = deque([word])\n",
                "    while len(to_visit) > 0:\n",
                "        source = to_visit.popleft() # visit next node\n",
                "        dist = graph.nodes[source]['dist']+1\n",
                "\n",
                "        if dist <= max_dist: # discover new nodes\n",
                "            for target, sim in model.most_similar(source, topn=top_n):\n",
                "                if target not in graph:\n",
                "                    to_visit.append(target)\n",
                "                    graph.add_node(target, dist=dist)\n",
                "                    graph.add_edge(source, target, sim=sim, dist=dist)\n",
                "    return graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plt_add_margin(pos, x_factor=0.1, y_factor=0.1):\n",
                "    # rescales the image s.t. all captions fit onto the canvas\n",
                "    x_values, y_values = zip(*pos.values())\n",
                "    x_max = max(x_values)\n",
                "    x_min = min(x_values)\n",
                "    y_max = max(y_values)\n",
                "    y_min = min(y_values)\n",
                "\n",
                "    x_margin = (x_max - x_min) * x_factor\n",
                "    y_margin = (y_max - y_min) * y_factor\n",
                "    # return (x_min - x_margin, x_max + x_margin), (y_min - y_margin, y_max + y_margin)\n",
                "\n",
                "    plt.xlim(x_min - x_margin, x_max + x_margin)\n",
                "    plt.ylim(y_min - y_margin, y_max + y_margin)\n",
                "\n",
                "def scale_weights(graph, minw=1, maxw=8):\n",
                "    # rescale similarity to interval [minw, maxw] for display\n",
                "    sims = [graph[s][t]['sim'] for (s, t) in graph.edges]\n",
                "    min_sim, max_sim = min(sims), max(sims)\n",
                "\n",
                "    for source, target in graph.edges:\n",
                "        sim = graph[source][target]['sim']\n",
                "        graph[source][target]['sim'] = (sim-min_sim)/(max_sim-min_sim)*(maxw-minw)+minw\n",
                "\n",
                "    return graph\n",
                "\n",
                "def solve_graphviz_problems(graph):\n",
                "    # Graphviz has problems with unicode\n",
                "    # this is to prevent errors during positioning\n",
                "    def clean(n):\n",
                "        n = n.replace(',', '')\n",
                "        n = n.encode().decode('ascii', errors='ignore')\n",
                "        n = re.sub(r'[{}\\[\\]]', '-', n)\n",
                "        n = re.sub(r'^\\-', '', n)\n",
                "        return n\n",
                "    \n",
                "    node_map = {n: clean(n) for n in graph.nodes}\n",
                "    # remove empty nodes\n",
                "    for n, m in node_map.items(): \n",
                "        if len(m) == 0:\n",
                "            graph.remove_node(n)\n",
                "    \n",
                "    return nx.relabel_nodes(graph, node_map)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from networkx.drawing.nx_pydot import graphviz_layout\n",
                "\n",
                "def plot_tree(graph, node_size=1000, font_size=12):\n",
                "    graph = solve_graphviz_problems(graph) ###\n",
                "\n",
                "    pos = graphviz_layout(graph, prog='twopi', root=list(graph.nodes)[0])\n",
                "    plt.figure(figsize=(10, 4), dpi=200) ###\n",
                "    plt.grid(b=None) ### hide box\n",
                "    plt.box(False) ### hide grid\n",
                "    plt_add_margin(pos) ### just for layout\n",
                "\n",
                "    colors = [graph.nodes[n]['dist'] for n in graph] # colorize by distance\n",
                "    nx.draw_networkx_nodes(graph, pos, node_size=node_size, node_color=colors, \n",
                "                           cmap='Set1', alpha=0.4)\n",
                "    nx.draw_networkx_labels(graph, pos, font_size=font_size)\n",
                "    scale_weights(graph) ### not in book\n",
                "    \n",
                "    for (n1, n2, sim) in graph.edges(data='sim'):\n",
                "         nx.draw_networkx_edges(graph, pos, [(n1, n2)], width=sim, alpha=0.2)\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = models['autos_w2v_sg_2']\n",
                "graph = sim_tree(model, 'noise', top_n=10, max_dist=3)\n",
                "plot_tree(graph, node_size=500, font_size=8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = models['autos_w2v_sg_30']\n",
                "graph = sim_tree(model, 'spark-plug', top_n=8, max_dist=2)\n",
                "plot_tree(graph, node_size=500, font_size=8)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Closing Remarks\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Further Reading\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {
                "height": "461px",
                "width": "526px"
            },
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {
                "height": "calc(100% - 180px)",
                "left": "10px",
                "top": "150px",
                "width": "215.575px"
            },
            "toc_section_display": true,
            "toc_window_display": true
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}